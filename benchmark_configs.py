"""æµ‹è¯•ä¸åŒé…ç½®ç»„åˆï¼Œæ‰¾åˆ°æœ€ä¼˜æ€§èƒ½é…ç½®"""

import torch
import time
from src.models.dit_model import DiTModel
from src.utils.config import load_config

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"æ€»æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\n")

# æµ‹è¯•é…ç½®
configs = [
    {"image_size": 256, "hidden_size": 512, "num_layers": 12, "num_heads": 8, "batch_size": 32},
    {"image_size": 256, "hidden_size": 768, "num_layers": 16, "num_heads": 12, "batch_size": 48},
    {"image_size": 256, "hidden_size": 768, "num_layers": 16, "num_heads": 12, "batch_size": 56},
    {"image_size": 256, "hidden_size": 768, "num_layers": 20, "num_heads": 16, "batch_size": 40},
    {"image_size": 256, "hidden_size": 1024, "num_layers": 16, "num_heads": 16, "batch_size": 32},
]

results = []

for i, cfg in enumerate(configs):
    print(f"æµ‹è¯•é…ç½® {i+1}/{len(configs)}:")
    print(f"  å›¾åƒ: {cfg['image_size']}, éšè—å±‚: {cfg['hidden_size']}, å±‚æ•°: {cfg['num_layers']}, å¤´æ•°: {cfg['num_heads']}, æ‰¹æ¬¡: {cfg['batch_size']}")
    
    try:
        torch.cuda.empty_cache()
        
        # åˆ›å»ºæ¨¡å‹
        latent_size = cfg['image_size'] // 8
        model = DiTModel(
            hidden_size=cfg['hidden_size'],
            num_layers=cfg['num_layers'],
            num_heads=cfg['num_heads'],
            input_size=latent_size,
        ).to(device)
        
        # æµ‹è¯•å‰å‘ä¼ æ’­
        latents = torch.randn(cfg['batch_size'], 4, latent_size, latent_size).to(device)
        timesteps = torch.randint(0, 1000, (cfg['batch_size'],)).to(device)
        text_emb = torch.randn(cfg['batch_size'], 512).to(device)
        
        # é¢„çƒ­
        with torch.amp.autocast(device_type="cuda"):
            _ = model(latents, timesteps, text_emb)
        
        torch.cuda.synchronize()
        
        # æµ‹è¯•é€Ÿåº¦
        start_time = time.time()
        for _ in range(10):
            with torch.amp.autocast(device_type="cuda"):
                _ = model(latents, timesteps, text_emb)
        torch.cuda.synchronize()
        elapsed = time.time() - start_time
        
        # æ£€æŸ¥æ˜¾å­˜
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        utilization = reserved / 24 * 100
        
        avg_time = elapsed / 10
        throughput = cfg['batch_size'] / avg_time
        
        results.append({
            **cfg,
            "allocated_gb": allocated,
            "reserved_gb": reserved,
            "utilization": utilization,
            "time_per_batch": avg_time,
            "throughput": throughput,
            "status": "OK"
        })
        
        print(f"  âœ“ æ˜¾å­˜: {reserved:.2f} GB ({utilization:.1f}%), é€Ÿåº¦: {avg_time:.3f}s/æ‰¹æ¬¡, åå: {throughput:.1f} æ ·æœ¬/s\n")
        
    except RuntimeError as e:
        results.append({
            **cfg,
            "status": "OOM",
            "error": str(e)[:50]
        })
        print(f"  âœ— OOM\n")
        torch.cuda.empty_cache()

# æ‰¾åˆ°æœ€ä¼˜é…ç½®
print("\n" + "="*80)
print("æœ€ä¼˜é…ç½®æ¨è:")
print("="*80)

# æŒ‰åˆ©ç”¨ç‡æ’åºï¼Œæ‰¾åˆ°åˆ©ç”¨ç‡é«˜ä¸”é€Ÿåº¦å¿«çš„
valid_results = [r for r in results if r.get("status") == "OK"]
if valid_results:
    # ç»¼åˆè¯„åˆ†ï¼šåˆ©ç”¨ç‡ * ååé‡
    for r in valid_results:
        r["score"] = r["utilization"] * r["throughput"]
    
    best = max(valid_results, key=lambda x: x["score"])
    
    print(f"\nğŸ† æ¨èé…ç½®ï¼ˆç»¼åˆè¯„åˆ†æœ€é«˜ï¼‰:")
    print(f"  å›¾åƒå°ºå¯¸: {best['image_size']}")
    print(f"  éšè—å±‚: {best['hidden_size']}")
    print(f"  å±‚æ•°: {best['num_layers']}")
    print(f"  æ³¨æ„åŠ›å¤´: {best['num_heads']}")
    print(f"  æ‰¹æ¬¡å¤§å°: {best['batch_size']}")
    print(f"  æ˜¾å­˜åˆ©ç”¨ç‡: {best['utilization']:.1f}% ({best['reserved_gb']:.2f} GB)")
    print(f"  è®­ç»ƒé€Ÿåº¦: {best['time_per_batch']:.3f} ç§’/æ‰¹æ¬¡")
    print(f"  ååé‡: {best['throughput']:.1f} æ ·æœ¬/ç§’")
    print(f"  ç»¼åˆè¯„åˆ†: {best['score']:.0f}")
    
    print(f"\nğŸ“Š æ‰€æœ‰æœ‰æ•ˆé…ç½®:")
    for r in sorted(valid_results, key=lambda x: x["score"], reverse=True):
        print(f"  æ‰¹æ¬¡{r['batch_size']:2d} | éšè—{r['hidden_size']:4d} | å±‚{r['num_layers']:2d} | å¤´{r['num_heads']:2d} | "
              f"æ˜¾å­˜{r['reserved_gb']:5.2f}GB ({r['utilization']:5.1f}%) | "
              f"é€Ÿåº¦{r['time_per_batch']:.3f}s | åå{r['throughput']:5.1f}æ ·æœ¬/s")

