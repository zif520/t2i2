# 03. 数据准备

本指南将帮助你准备用于训练 DiT 模型的数据集。

## 数据格式要求

### 基本要求

DiT 模型需要**图像-文本对**数据，即每张图像对应一个文本描述。

**数据格式：**
- **图像**：RGB 格式，推荐尺寸 256x256 或更大
- **文本**：英文描述，长度建议在 10-100 个词之间

### 示例数据

```
图像: cat.jpg
文本: "a cat sitting on a chair"

图像: landscape.jpg
文本: "a beautiful landscape with mountains and a lake"
```

## 数据集选择

### 1. COCO 数据集（推荐用于学习）

**优点：**
- 公开可用
- 质量较高
- 标注完整

**获取方式：**
- 使用 Hugging Face datasets 库自动下载
- 或从 [COCO 官网](https://cocodataset.org/) 下载

**使用子集：**
- 为了快速训练，我们使用 1000-5000 张图像的小子集

### 2. 自定义数据集

**适用场景：**
- 特定领域的图像生成
- 特定风格的图像生成

**准备步骤：**
1. 收集图像和对应的文本描述
2. 整理为指定格式
3. 使用数据准备脚本处理

## 使用数据准备脚本

### 准备 COCO 子集

```bash
python src/scripts/prepare_data.py \
    --type coco \
    --output ./data/coco_subset \
    --num_samples 5000
```

### 准备自定义数据集

**数据目录结构：**

```
custom_data/
├── image1.jpg
├── image1.txt
├── image2.jpg
├── image2.txt
└── ...
```

**运行脚本：**

```bash
python src/scripts/prepare_data.py \
    --type custom \
    --input ./custom_data \
    --output ./data/custom
```

## 数据格式说明

### JSON 格式（推荐）

项目使用 JSON 格式存储元数据：

```json
[
    {
        "image": "images/image_000001.jpg",
        "text": "a cat sitting on a chair"
    },
    {
        "image": "images/image_000002.jpg",
        "text": "a beautiful landscape"
    }
]
```

**目录结构：**

```
data/
├── metadata.json
└── images/
    ├── image_000001.jpg
    ├── image_000002.jpg
    └── ...
```

### 文本文件格式

每张图像对应一个同名的文本文件：

```
image1.jpg  →  image1.txt
image2.jpg  →  image2.txt
```

## 数据预处理

### 图像预处理

项目会自动进行以下预处理：

1. **调整尺寸**：缩放到目标尺寸（默认 256x256）
2. **中心裁剪**：确保图像为正方形
3. **归一化**：将像素值从 [0, 255] 归一化到 [-1, 1]
4. **数据增强**（训练时）：
   - 随机水平翻转

### 文本预处理

1. **分词**：使用 CLIP tokenizer 进行分词
2. **填充/截断**：统一长度为 77（CLIP 最大长度）
3. **编码**：转换为 token IDs

## 数据集类使用

### 创建数据集

```python
from src.data.dataset import TextImageDataset

dataset = TextImageDataset(
    dataset_name="coco",  # 或 "custom"
    dataset_path="./data/coco_subset",  # 自定义数据集路径
    image_size=256,
    num_samples=5000,
    is_train=True,
)
```

### 数据加载器

```python
from torch.utils.data import DataLoader

dataloader = DataLoader(
    dataset,
    batch_size=4,
    shuffle=True,
    num_workers=2,
)
```

## 数据质量建议

### 图像质量

- **分辨率**：至少 256x256，推荐 512x512
- **格式**：JPG 或 PNG
- **内容**：清晰、无模糊
- **多样性**：包含不同场景、物体、风格

### 文本质量

- **描述性**：准确描述图像内容
- **详细程度**：包含主要物体、颜色、动作等
- **长度**：10-100 个词
- **语言**：英文（CLIP 对英文支持最好）

## 小数据集配置

为了适配 RTX 4090 和快速训练，我们使用小数据集：

### 推荐配置

- **样本数量**：1000-5000 张图像
- **图像尺寸**：256x256
- **批次大小**：4（配合梯度累积）

### 为什么使用小数据集？

1. **快速迭代**：可以快速验证代码和模型
2. **资源友好**：适合单卡训练
3. **学习目的**：理解流程比追求性能更重要

## 数据验证

### 检查数据

```python
from src.data.dataset import TextImageDataset

dataset = TextImageDataset(
    dataset_name="custom",
    dataset_path="./data/custom",
    image_size=256,
)

# 检查样本
sample = dataset[0]
print("图像形状:", sample["pixel_values"].shape)
print("文本:", sample["text"])
print("文本 ID 形状:", sample["input_ids"].shape)
```

### 可视化数据

```python
from PIL import Image
import torch

sample = dataset[0]
image = sample["pixel_values"]
text = sample["text"]

# 反归一化图像
image = (image + 1.0) / 2.0
image = image.clamp(0, 1)

# 转换为 PIL 图像
from torchvision.transforms import ToPILImage
to_pil = ToPILImage()
pil_image = to_pil(image)

# 显示
pil_image.show()
print("文本:", text)
```

## 常见问题

### 1. 数据加载失败

**问题**：无法加载图像或文本

**解决方案：**
- 检查文件路径是否正确
- 确认图像文件存在且可读
- 检查 JSON 格式是否正确

### 2. 内存不足

**问题**：加载数据时内存不足

**解决方案：**
- 减少 `num_samples`
- 减小 `image_size`
- 减少 `num_workers`

### 3. 文本编码错误

**问题**：文本无法正确编码

**解决方案：**
- 检查文本是否为英文
- 确认文本长度合理
- 检查特殊字符

## 下一步

数据准备完成后，可以：

1. 阅读 [模型架构](./04-模型架构.md) 了解模型结构
2. 阅读 [训练流程](./05-训练流程.md) 开始训练
3. 查看配置文件调整数据相关参数

---

**数据准备完成！让我们开始训练吧！** 🚀



