# 04. æ¨¡å‹æ¶æ„

æœ¬æŒ‡å—æ·±å…¥è§£æ DiT (Diffusion Transformer) æ¨¡å‹çš„æ¶æ„è®¾è®¡ï¼ŒåŸºäºé¡¹ç›®å®é™…ä»£ç ã€‚

## ğŸ—ï¸ æ•´ä½“æ¶æ„

DiT æ¨¡å‹ç”±ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ç»„æˆï¼š

```
è¾“å…¥å›¾åƒ â†’ VAE ç¼–ç å™¨ â†’ æ½œåœ¨è¡¨ç¤º â†’ DiT æ¨¡å‹ â†’ æ½œåœ¨è¡¨ç¤º â†’ VAE è§£ç å™¨ â†’ è¾“å‡ºå›¾åƒ
æ–‡æœ¬æè¿° â†’ CLIP æ–‡æœ¬ç¼–ç å™¨ â†’ æ–‡æœ¬åµŒå…¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶

1. **VAE ç¼–ç å™¨** (`VAEEncoder`) - å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´
2. **DiT æ¨¡å‹** (`DiTModel`) - åœ¨æ½œåœ¨ç©ºé—´ä¸­é¢„æµ‹å™ªå£°
3. **VAE è§£ç å™¨** (`VAEDecoder`) - å°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºå›¾åƒ
4. **CLIP æ–‡æœ¬ç¼–ç å™¨** - å°†æ–‡æœ¬æè¿°ç¼–ç ä¸ºåµŒå…¥å‘é‡

## ğŸ”§ VAE æ¨¡å—

### VAE ç¼–ç å™¨ (`VAEEncoder`)

**ä½ç½®**: `src/models/vae_model.py`

**åŠŸèƒ½**: å°† RGB å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´ï¼ˆé™ä½è®¡ç®—å¤æ‚åº¦ï¼‰

```python
class VAEEncoder:
    def __init__(
        self,
        pretrained_model_name: str = "runwayml/stable-diffusion-v1-5",
        use_slicing: bool = False,
    ):
        # åŠ è½½é¢„è®­ç»ƒçš„ VAE æ¨¡å‹
        self.vae = AutoencoderKL.from_pretrained(
            pretrained_model_name,
            subfolder="vae" if "stable-diffusion" in pretrained_model_name else None,
        )
```

**è¾“å…¥**: RGB å›¾åƒï¼Œå½¢çŠ¶ `(B, 3, H, W)`
**è¾“å‡º**: æ½œåœ¨è¡¨ç¤ºï¼Œå½¢çŠ¶ `(B, 4, H/8, W/8)`

**å…³é”®å‚æ•°**:
- `pretrained_model_name`: é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼ˆé»˜è®¤: `"runwayml/stable-diffusion-v1-5"`ï¼‰
- `use_slicing`: æ˜¯å¦ä½¿ç”¨åˆ‡ç‰‡ï¼ˆèŠ‚çœæ˜¾å­˜ï¼Œä½†ä¼šé™ä½é€Ÿåº¦ï¼‰

### VAE è§£ç å™¨ (`VAEDecoder`)

**åŠŸèƒ½**: å°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸º RGB å›¾åƒ

**è¾“å…¥**: æ½œåœ¨è¡¨ç¤ºï¼Œå½¢çŠ¶ `(B, 4, H/8, W/8)`
**è¾“å‡º**: RGB å›¾åƒï¼Œå½¢çŠ¶ `(B, 3, H, W)`

## ğŸ§  DiT æ¨¡å‹æ¶æ„

### æ•´ä½“ç»“æ„

**ä½ç½®**: `src/models/dit_model.py`

DiT æ¨¡å‹é‡‡ç”¨ Transformer æ¶æ„ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œæ‰©æ•£è¿‡ç¨‹ï¼š

```python
class DiTModel(nn.Module):
    def __init__(
        self,
        hidden_size: int = 768,      # éšè—å±‚ç»´åº¦
        num_layers: int = 16,         # Transformer å±‚æ•°
        num_heads: int = 12,          # æ³¨æ„åŠ›å¤´æ•°
        patch_size: int = 2,          # Patch å¤§å°
        in_channels: int = 4,         # è¾“å…¥é€šé“æ•°ï¼ˆVAE æ½œåœ¨ç©ºé—´ï¼‰
        out_channels: int = 4,        # è¾“å‡ºé€šé“æ•°
        attention_head_dim: int = 64, # æ³¨æ„åŠ›å¤´ç»´åº¦
        mlp_ratio: float = 4.0,       # MLP æ‰©å±•æ¯”ä¾‹
        dropout: float = 0.1,         # Dropout ç‡
        input_size: int = 32,         # è¾“å…¥å°ºå¯¸ï¼ˆæ½œåœ¨ç©ºé—´ï¼‰
    ):
```

### æ ¸å¿ƒç»„ä»¶

#### 1. Patch åµŒå…¥ (`PatchEmbed`)

å°†æ½œåœ¨è¡¨ç¤ºåˆ†å‰²ä¸º patches å¹¶åµŒå…¥ï¼š

```python
class PatchEmbed(nn.Module):
    def __init__(self, img_size, patch_size, in_channels, hidden_size):
        # å°†å›¾åƒåˆ†å‰²ä¸º patches
        # ä¾‹å¦‚: 256x256 â†’ 128x128 patches (patch_size=2)
```

**åŠŸèƒ½**:
- å°† `(B, 4, H, W)` çš„æ½œåœ¨è¡¨ç¤ºåˆ†å‰²ä¸º patches
- æ¯ä¸ª patch è½¬æ¢ä¸ºåµŒå…¥å‘é‡
- è¾“å‡ºå½¢çŠ¶: `(B, N, hidden_size)`ï¼Œå…¶ä¸­ `N = (H/patch_size) * (W/patch_size)`

#### 2. æ—¶é—´æ­¥åµŒå…¥ (`TimestepEmbedder`)

å°†æ‰©æ•£æ—¶é—´æ­¥ç¼–ç ä¸ºåµŒå…¥å‘é‡ï¼š

```python
class TimestepEmbedder(nn.Module):
    def __init__(self, hidden_size: int):
        self.mlp = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 4),
            nn.SiLU(),
            nn.Linear(hidden_size * 4, hidden_size),
        )
```

**åŠŸèƒ½**:
- ä½¿ç”¨æ­£å¼¦ä½ç½®ç¼–ç ç”Ÿæˆæ—¶é—´æ­¥åµŒå…¥
- é€šè¿‡ MLP è¿›ä¸€æ­¥å¤„ç†
- è¾“å‡ºå½¢çŠ¶: `(B, hidden_size)`

#### 3. æ ‡ç­¾åµŒå…¥ (`LabelEmbedder` / `y_embedder`)

å°†æ¡ä»¶ä¿¡æ¯ï¼ˆæ–‡æœ¬åµŒå…¥ï¼‰ç¼–ç ï¼š

```python
# åŠ¨æ€åˆå§‹åŒ–ï¼Œé€‚é…ä¸åŒçš„æ–‡æœ¬ç¼–ç å™¨è¾“å‡ºç»´åº¦
self.y_embedder = nn.Linear(text_dim, hidden_size)
```

**åŠŸèƒ½**:
- å°† CLIP æ–‡æœ¬åµŒå…¥ï¼ˆ512 ç»´ï¼‰æŠ•å½±åˆ°æ¨¡å‹éšè—ç»´åº¦ï¼ˆ768 ç»´ï¼‰
- æ”¯æŒä¸åŒçš„æ–‡æœ¬ç¼–ç å™¨

#### 4. æ³¨æ„åŠ›æ¨¡å— (`Attention`)

å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼š

```python
class Attention(nn.Module):
    def __init__(
        self,
        dim: int,
        num_heads: int = 8,
        head_dim: Optional[int] = None,
        qkv_bias: bool = False,
    ):
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.proj = nn.Linear(dim, dim)
```

**åŠŸèƒ½**:
- è®¡ç®— Queryã€Keyã€Value
- å¤šå¤´æ³¨æ„åŠ›è®¡ç®—
- è¾“å‡ºæŠ•å½±

#### 5. DiT å— (`DiTBlock`)

ç»“åˆæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œçš„ Transformer å—ï¼š

```python
class DiTBlock(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        mlp_ratio: float = 4.0,
        attention_head_dim: Optional[int] = None,
    ):
        self.norm1 = nn.LayerNorm(hidden_size)
        self.attn = Attention(hidden_size, num_heads, attention_head_dim)
        self.norm2 = nn.LayerNorm(hidden_size)
        self.mlp = Mlp(hidden_size, hidden_size * mlp_ratio)
```

**ç»“æ„**:
```
è¾“å…¥ â†’ LayerNorm â†’ Attention â†’ æ®‹å·®è¿æ¥
  â†“
LayerNorm â†’ MLP â†’ æ®‹å·®è¿æ¥ â†’ è¾“å‡º
```

#### 6. æœ€ç»ˆå±‚ (`FinalLayer`)

å°† Transformer è¾“å‡ºè½¬æ¢ä¸ºå™ªå£°é¢„æµ‹ï¼š

```python
class FinalLayer(nn.Module):
    def __init__(self, hidden_size, out_channels):
        self.norm_final = nn.LayerNorm(hidden_size)
        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels)
```

**åŠŸèƒ½**:
- å°† `(B, N, hidden_size)` è½¬æ¢ä¸º `(B, N, patch_sizeÂ² * out_channels)`
- é‡å¡‘ä¸º `(B, out_channels, H, W)`

### å‰å‘ä¼ æ’­æµç¨‹

```python
def forward(
    self,
    x: torch.Tensor,           # æ½œåœ¨è¡¨ç¤º (B, 4, H, W)
    timestep: torch.Tensor,     # æ—¶é—´æ­¥ (B,)
    y: torch.Tensor,            # æ–‡æœ¬åµŒå…¥ (B, text_dim)
) -> torch.Tensor:
    # 1. Patch åµŒå…¥
    x = self.patch_embed(x)  # (B, N, hidden_size)
    
    # 2. æ·»åŠ ä½ç½®ç¼–ç 
    x = x + self.pos_embed
    
    # 3. æ—¶é—´æ­¥åµŒå…¥
    t = self.t_embedder(timestep)  # (B, hidden_size)
    
    # 4. æ–‡æœ¬åµŒå…¥
    y = self.y_embedder(y)  # (B, hidden_size)
    
    # 5. æ¡ä»¶ä¿¡æ¯ï¼ˆæ—¶é—´æ­¥ + æ–‡æœ¬ï¼‰
    c = t + y  # (B, hidden_size)
    
    # 6. é€šè¿‡ Transformer å±‚
    for block in self.blocks:
        x = block(x, c)  # æ¯ä¸ªå—éƒ½ä½¿ç”¨æ¡ä»¶ä¿¡æ¯
    
    # 7. æœ€ç»ˆå±‚
    x = self.final_layer(x, c)
    
    return x  # é¢„æµ‹çš„å™ªå£° (B, 4, H, W)
```

## ğŸ“ æ¨¡å‹é…ç½®

### å½“å‰é…ç½®ï¼ˆRTX 4090 ä¼˜åŒ–ï¼‰

```yaml
model:
  hidden_size: 768          # éšè—å±‚ç»´åº¦
  num_layers: 16            # Transformer å±‚æ•°
  num_heads: 12             # æ³¨æ„åŠ›å¤´æ•°
  patch_size: 2             # Patch å¤§å°
  in_channels: 4            # VAE æ½œåœ¨ç©ºé—´é€šé“æ•°
  out_channels: 4
  attention_head_dim: 64    # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
  mlp_ratio: 4.0            # MLP æ‰©å±•æ¯”ä¾‹
  dropout: 0.1              # Dropout ç‡
```

### å‚æ•°è®¡ç®—

- **è¾“å…¥å°ºå¯¸**: 256x256 å›¾åƒ â†’ VAE ç¼–ç  â†’ 32x32 æ½œåœ¨è¡¨ç¤º
- **Patch æ•°é‡**: `(32/2) * (32/2) = 16 * 16 = 256` patches
- **æ¨¡å‹å‚æ•°é‡**: çº¦ 100M-200Mï¼ˆå–å†³äºé…ç½®ï¼‰

## ğŸ”„ æ‰©æ•£è¿‡ç¨‹

### è®­ç»ƒè¿‡ç¨‹

1. **ç¼–ç **: å›¾åƒ â†’ VAE ç¼–ç å™¨ â†’ æ½œåœ¨è¡¨ç¤º `zâ‚€`
2. **åŠ å™ª**: `zâ‚€` â†’ æ·»åŠ å™ªå£° â†’ `zâ‚œ`ï¼ˆæ—¶é—´æ­¥ `t`ï¼‰
3. **é¢„æµ‹**: DiT æ¨¡å‹é¢„æµ‹å™ªå£° `Îµ_pred`
4. **æŸå¤±**: `MSE(Îµ_pred, Îµ_true)`

### æ¨ç†è¿‡ç¨‹

1. **åˆå§‹åŒ–**: éšæœºå™ªå£° `z_T`
2. **å»å™ª**: é€æ­¥å»å™ª `z_T â†’ z_{T-1} â†’ ... â†’ zâ‚€`
3. **è§£ç **: `zâ‚€` â†’ VAE è§£ç å™¨ â†’ å›¾åƒ

## ğŸ¯ æ¡ä»¶ç”Ÿæˆ

### æ–‡æœ¬æ¡ä»¶

- **æ–‡æœ¬ç¼–ç **: CLIP æ–‡æœ¬ç¼–ç å™¨å°†æ–‡æœ¬è½¬æ¢ä¸ºåµŒå…¥å‘é‡
- **æ¡ä»¶æ³¨å…¥**: æ–‡æœ¬åµŒå…¥ä¸æ—¶é—´æ­¥åµŒå…¥ç›¸åŠ ï¼Œæ³¨å…¥åˆ°æ¯ä¸ª Transformer å—
- **Classifier-Free Guidance**: æ¨ç†æ—¶ä½¿ç”¨å¼•å¯¼å¼ºåº¦æ§åˆ¶ç”Ÿæˆè´¨é‡

### å®ç°ç»†èŠ‚

```python
# åœ¨ DiTBlock ä¸­
def forward(self, x, c):
    # c åŒ…å«æ—¶é—´æ­¥å’Œæ–‡æœ¬ä¿¡æ¯
    x = x + self.adaLN_modulation(c)  # è‡ªé€‚åº”å±‚å½’ä¸€åŒ–
    x = x + self.attn(self.norm1(x))
    x = x + self.mlp(self.norm2(x))
    return x
```

## ğŸ“Š æ¨¡å‹è§„æ¨¡

### å°æ¨¡å‹é…ç½®ï¼ˆå­¦ä¹ ç”¨ï¼‰

```yaml
hidden_size: 384
num_layers: 8
num_heads: 6
```

### ä¸­ç­‰æ¨¡å‹é…ç½®ï¼ˆå½“å‰ï¼‰

```yaml
hidden_size: 768
num_layers: 16
num_heads: 12
```

### å¤§æ¨¡å‹é…ç½®ï¼ˆç ”ç©¶ç”¨ï¼‰

```yaml
hidden_size: 1024
num_layers: 24
num_heads: 16
```

## ğŸ” ä»£ç ä½ç½®

- **DiT æ¨¡å‹**: `src/models/dit_model.py`
- **VAE æ¨¡å‹**: `src/models/vae_model.py`
- **è®­ç»ƒå™¨**: `src/training/trainer.py`
- **ç”Ÿæˆå™¨**: `src/inference/generator.py`

## ğŸ“ ä¸‹ä¸€æ­¥

- ğŸ“– [05. è®­ç»ƒæŒ‡å—](./05-è®­ç»ƒæŒ‡å—.md) - äº†è§£è®­ç»ƒæµç¨‹
- ğŸ“– [06. æ¨ç†ä½¿ç”¨](./06-æ¨ç†ä½¿ç”¨.md) - ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›¾åƒ

---

**æ·±å…¥ç†è§£**: é˜…è¯» `src/models/dit_model.py` æºç ï¼Œç†è§£æ¯ä¸ªç»„ä»¶çš„å®ç°ç»†èŠ‚ï¼
