# 05. è®­ç»ƒæŒ‡å—

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç» DiT æ¨¡å‹çš„è®­ç»ƒæµç¨‹ï¼ŒåŸºäºå®é™…ä»£ç å®ç°ã€‚

## ğŸ¯ è®­ç»ƒæµç¨‹æ¦‚è¿°

è®­ç»ƒæµç¨‹ç”±ä»¥ä¸‹æ­¥éª¤ç»„æˆï¼š

1. **æ•°æ®åŠ è½½** - åŠ è½½å›¾åƒ-æ–‡æœ¬å¯¹æ•°æ®
2. **æ¨¡å‹åˆå§‹åŒ–** - åˆ›å»º DiT æ¨¡å‹ã€VAE ç¼–ç å™¨ã€æ–‡æœ¬ç¼–ç å™¨
3. **è®­ç»ƒå¾ªç¯** - è¿­ä»£è®­ç»ƒï¼Œä¼˜åŒ–æ¨¡å‹å‚æ•°
4. **æ£€æŸ¥ç‚¹ä¿å­˜** - å®šæœŸä¿å­˜æ¨¡å‹çŠ¶æ€

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€è®­ç»ƒå‘½ä»¤

```bash
python src/scripts/train.py --config configs/train_config.yaml
```

### æ¢å¤è®­ç»ƒ

```bash
python src/scripts/train.py \
    --config configs/train_config.yaml \
    --resume ./outputs/checkpoint-epoch-10
```

## ğŸ“‹ è®­ç»ƒè„šæœ¬å‚æ•°

### `train.py` å‚æ•°

**ä½ç½®**: `src/scripts/train.py`

```bash
python src/scripts/train.py \
    --config <é…ç½®æ–‡ä»¶è·¯å¾„>    # é»˜è®¤: configs/train_config.yaml
    --resume <æ£€æŸ¥ç‚¹è·¯å¾„>      # å¯é€‰: æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹
```

### é…ç½®æ–‡ä»¶å‚æ•°

**ä½ç½®**: `configs/train_config.yaml`

ä¸»è¦é…ç½®é¡¹ï¼š

```yaml
data:
  dataset_name: "cub"           # æ•°æ®é›†åç§°: "coco", "cub", "custom"
  dataset_path: "./data/cub_subset"  # æ•°æ®é›†è·¯å¾„
  num_samples: 5000              # ä½¿ç”¨çš„æ ·æœ¬æ•°
  image_size: 256                # å›¾åƒå°ºå¯¸

model:
  hidden_size: 768              # éšè—å±‚ç»´åº¦
  num_layers: 16                 # Transformer å±‚æ•°
  num_heads: 12                  # æ³¨æ„åŠ›å¤´æ•°

training:
  batch_size: 96                 # æ‰¹æ¬¡å¤§å°
  learning_rate: 0.0001          # å­¦ä¹ ç‡
  num_epochs: 1000               # è®­ç»ƒè½®æ•°
  mixed_precision: "bf16"        # æ··åˆç²¾åº¦: "fp16", "bf16"
  save_every_n_epochs: 10        # æ¯ N ä¸ª epoch ä¿å­˜ä¸€æ¬¡
```

## ğŸ”„ è®­ç»ƒæµç¨‹è¯¦è§£

### 1. åˆå§‹åŒ–é˜¶æ®µ

**ä»£ç ä½ç½®**: `src/scripts/train.py` (main å‡½æ•°)

```python
# 1. åŠ è½½é…ç½®
config = load_config(args.config)

# 2. è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 3. åŠ è½½æ–‡æœ¬ç¼–ç å™¨
text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")
text_encoder = text_encoder.to(device)
text_encoder.eval()

# 4. åŠ è½½ VAE ç¼–ç å™¨
vae_encoder = VAEEncoder(
    pretrained_model_name="runwayml/stable-diffusion-v1-5",
    use_slicing=False,
)
vae_encoder = vae_encoder.to(device)

# 5. åˆ›å»ºæ•°æ®é›†
dataset = TextImageDataset(
    dataset_name=config.data.get("dataset_name", "coco"),
    dataset_path=config.data.get("dataset_path"),
    image_size=config.data.get("image_size", 256),
    num_samples=config.data.get("num_samples"),
    is_train=True,
)

# 6. åˆ›å»ºæ•°æ®åŠ è½½å™¨
dataloader = DataLoader(
    dataset,
    batch_size=config.training.get("batch_size", 32),
    shuffle=True,
    num_workers=config.training.get("num_workers", 4),
    pin_memory=True,
    prefetch_factor=config.training.get("prefetch_factor", 2),
    persistent_workers=True,
    drop_last=True,
)

# 7. åˆ›å»º DiT æ¨¡å‹
model = DiTModel(
    hidden_size=config.model.get("hidden_size", 768),
    num_layers=config.model.get("num_layers", 16),
    num_heads=config.model.get("num_heads", 12),
    # ... å…¶ä»–å‚æ•°
)

# 8. åˆ›å»ºè®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    vae_encoder=vae_encoder,
    text_encoder=text_encoder,
    train_dataloader=dataloader,
    config=config,
)
```

### 2. è®­ç»ƒå¾ªç¯

**ä»£ç ä½ç½®**: `src/training/trainer.py` (train æ–¹æ³•)

```python
def train(self):
    # 1. å‡†å¤‡è®­ç»ƒ
    self.model.train()
    self.accelerator.prepare(
        self.model,
        self.optimizer,
        self.train_dataloader,
        self.lr_scheduler,
    )
    
    # 2. åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœæ¢å¤è®­ç»ƒï¼‰
    if resume_checkpoint:
        self.load_checkpoint(resume_checkpoint)
    
    # 3. è®­ç»ƒå¾ªç¯
    for epoch in range(start_epoch, num_epochs):
        for batch in self.train_dataloader:
            # 3.1 å‰å‘ä¼ æ’­
            loss = self.training_step(batch)
            
            # 3.2 åå‘ä¼ æ’­
            self.accelerator.backward(loss)
            
            # 3.3 æ¢¯åº¦è£å‰ª
            if self.config.training.get("max_grad_norm"):
                self.accelerator.clip_grad_norm_(
                    self.model.parameters(),
                    self.config.training.get("max_grad_norm"),
                )
            
            # 3.4 ä¼˜åŒ–å™¨æ­¥è¿›
            self.optimizer.step()
            self.optimizer.zero_grad(set_to_none=True)
            
            # 3.5 å­¦ä¹ ç‡è°ƒåº¦
            if self.lr_scheduler:
                self.lr_scheduler.step()
            
            # 3.6 æ—¥å¿—è®°å½•
            if global_step % logging_steps == 0:
                logger.info(f"Step {global_step}, Loss: {loss.item():.4f}")
        
        # 4. ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % save_every_n_epochs == 0:
            self.save_checkpoint(output_dir / f"checkpoint-epoch-{epoch+1}")
```

### 3. è®­ç»ƒæ­¥éª¤è¯¦è§£

**ä»£ç ä½ç½®**: `src/training/trainer.py` (training_step æ–¹æ³•)

```python
def training_step(self, batch: Dict[str, torch.Tensor]) -> torch.Tensor:
    """
    å•æ­¥è®­ç»ƒ
    
    æµç¨‹:
    1. ç¼–ç å›¾åƒåˆ°æ½œåœ¨ç©ºé—´
    2. æ·»åŠ å™ªå£°
    3. é¢„æµ‹å™ªå£°
    4. è®¡ç®—æŸå¤±
    """
    images = batch["image"].to(self.device)  # (B, 3, H, W)
    text_embeddings = batch["text_embeddings"].to(self.device)  # (B, seq_len, dim)
    
    # 1. VAE ç¼–ç 
    with torch.amp.autocast(device_type="cuda", dtype=torch.bfloat16):
        latents = self.vae_encoder.encode(images)  # (B, 4, H/8, W/8)
    
    # 2. é‡‡æ ·æ—¶é—´æ­¥
    timesteps = torch.randint(
        0,
        self.noise_scheduler.config.num_train_timesteps,
        (latents.shape[0],),
        device=latents.device,
    )
    
    # 3. æ·»åŠ å™ªå£°
    noise = torch.randn_like(latents)
    noisy_latents = self.noise_scheduler.add_noise(latents, noise, timesteps)
    
    # 4. é¢„æµ‹å™ªå£°
    with torch.amp.autocast(device_type="cuda", dtype=torch.bfloat16):
        noise_pred = self.model(
            noisy_latents,
            timesteps,
            text_embeddings,
        )
    
    # 5. è®¡ç®—æŸå¤±
    loss = self.criterion(noise_pred, noise)
    
    return loss
```

## âš™ï¸ è®­ç»ƒé…ç½®è¯¦è§£

### æ‰¹æ¬¡å¤§å°å’Œæ¢¯åº¦ç´¯ç§¯

```yaml
training:
  batch_size: 96                 # å®é™…æ‰¹æ¬¡å¤§å°
  gradient_accumulation_steps: 1 # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
  # æœ‰æ•ˆæ‰¹æ¬¡å¤§å° = batch_size * gradient_accumulation_steps
```

**ä¼˜åŒ–å»ºè®®**:
- RTX 4090 (24GB): `batch_size: 96`
- æ˜¾å­˜ä¸è¶³æ—¶: å‡å° `batch_size`ï¼Œå¢åŠ  `gradient_accumulation_steps`

### æ··åˆç²¾åº¦è®­ç»ƒ

```yaml
training:
  mixed_precision: "bf16"  # æˆ– "fp16"
```

**é€‰æ‹©å»ºè®®**:
- **BF16** (æ¨è): RTX 4090 åŸç”Ÿæ”¯æŒï¼Œæ›´ç¨³å®š
- **FP16**: å…¼å®¹æ€§æ›´å¥½ï¼Œä½†å¯èƒ½ä¸ç¨³å®š

### å­¦ä¹ ç‡è°ƒåº¦

```yaml
lr_scheduler:
  type: "cosine"        # ä½™å¼¦é€€ç«
  warmup_steps: 500    # é¢„çƒ­æ­¥æ•°
```

**å­¦ä¹ ç‡å˜åŒ–**:
- å‰ 500 æ­¥: çº¿æ€§å¢åŠ åˆ° `learning_rate`
- ä¹‹å: ä½™å¼¦é€€ç«åˆ° 0

### æ£€æŸ¥ç‚¹ä¿å­˜

```yaml
training:
  save_every_n_epochs: 10  # æ¯ 10 ä¸ª epoch ä¿å­˜ä¸€æ¬¡
  save_steps: 500          # æ¯ 500 æ­¥ä¿å­˜ä¸€æ¬¡ï¼ˆå¯é€‰ï¼‰
```

**ä¿å­˜å†…å®¹**:
- `model.pt` - æ¨¡å‹æƒé‡
- `optimizer.pt` - ä¼˜åŒ–å™¨çŠ¶æ€
- `scheduler.pt` - å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
- `training_state.json` - è®­ç»ƒçŠ¶æ€ï¼ˆepoch, step ç­‰ï¼‰

## ğŸ”„ æ–­ç‚¹ç»­ä¼ 

### æ¢å¤è®­ç»ƒ

```bash
python src/scripts/train.py \
    --config configs/train_config.yaml \
    --resume ./outputs/checkpoint-epoch-10
```

**æ¢å¤å†…å®¹**:
- æ¨¡å‹æƒé‡
- ä¼˜åŒ–å™¨çŠ¶æ€
- å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
- è®­ç»ƒè¿›åº¦ï¼ˆepoch, stepï¼‰

**ä»£ç å®ç°**: `src/training/trainer.py` (load_checkpoint æ–¹æ³•)

```python
def load_checkpoint(self, checkpoint_dir: Path):
    # 1. åŠ è½½æ¨¡å‹
    model_path = checkpoint_dir / "model.pt"
    state_dict = torch.load(model_path, map_location=self.device)
    self.model.load_state_dict(state_dict)
    
    # 2. åŠ è½½ä¼˜åŒ–å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    optimizer_path = checkpoint_dir / "optimizer.pt"
    if optimizer_path.exists():
        optimizer_state = torch.load(optimizer_path, map_location=self.device)
        self.optimizer.load_state_dict(optimizer_state)
    
    # 3. åŠ è½½è®­ç»ƒçŠ¶æ€
    state_path = checkpoint_dir / "training_state.json"
    if state_path.exists():
        with open(state_path, 'r') as f:
            state = json.load(f)
        self.current_epoch = state.get("epoch", 0)
        self.global_step = state.get("global_step", 0)
```

## ğŸ“Š è®­ç»ƒç›‘æ§

### æ—¥å¿—è¾“å‡º

è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨ `./outputs/train.log`ï¼ŒåŒ…å«ï¼š
- è®­ç»ƒè¿›åº¦
- Loss å€¼
- å­¦ä¹ ç‡
- GPU ä½¿ç”¨æƒ…å†µ

### TensorBoardï¼ˆå¯é€‰ï¼‰

å¦‚æœå®‰è£…äº† TensorBoardï¼Œå¯ä»¥å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ï¼š

```bash
tensorboard --logdir ./outputs
```

## ğŸ¯ è®­ç»ƒæŠ€å·§

### 1. å­¦ä¹ ç‡é€‰æ‹©

- **åˆå§‹å­¦ä¹ ç‡**: `1e-4` (æ¨è)
- **å°æ¨¡å‹**: å¯ä»¥å°è¯• `2e-4`
- **å¤§æ¨¡å‹**: å¯ä»¥å°è¯• `5e-5`

### 2. è®­ç»ƒè½®æ•°

- **å°æ•°æ®é›†** (5000 æ ·æœ¬): 200-400 epochs
- **ä¸­ç­‰æ•°æ®é›†** (50000 æ ·æœ¬): 100-200 epochs
- **å¤§æ•°æ®é›†** (500000+ æ ·æœ¬): 50-100 epochs

### 3. æ‰¹æ¬¡å¤§å°ä¼˜åŒ–

æ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼š

```yaml
# RTX 4090 (24GB) - æœ€ä¼˜é…ç½®
batch_size: 96

# RTX 3090 (24GB)
batch_size: 64

# RTX 3080 (10GB)
batch_size: 16
gradient_accumulation_steps: 4
```

### 4. æ•°æ®åŠ è½½ä¼˜åŒ–

```yaml
training:
  num_workers: 8              # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
  prefetch_factor: 4          # é¢„å–å› å­
  persistent_workers: true    # ä¿æŒå·¥ä½œè¿›ç¨‹
  drop_last: true            # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„æ‰¹æ¬¡
```

## ğŸ› å¸¸è§è®­ç»ƒé—®é¢˜

### é—®é¢˜ 1: æ˜¾å­˜ä¸è¶³

**ç—‡çŠ¶**: `RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
- å‡å° `batch_size`
- å¢åŠ  `gradient_accumulation_steps`
- å¯ç”¨ VAE åˆ‡ç‰‡: `use_slicing: true`

### é—®é¢˜ 2: Loss ä¸ä¸‹é™

**å¯èƒ½åŸå› **:
- å­¦ä¹ ç‡è¿‡å¤§æˆ–è¿‡å°
- æ•°æ®è´¨é‡é—®é¢˜
- æ¨¡å‹é…ç½®ä¸å½“

**è§£å†³æ–¹æ¡ˆ**:
- è°ƒæ•´å­¦ä¹ ç‡
- æ£€æŸ¥æ•°æ®è´¨é‡
- ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰

### é—®é¢˜ 3: è®­ç»ƒä¸­æ–­

**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨ `--resume` å‚æ•°æ¢å¤è®­ç»ƒ
- æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³
- æ£€æŸ¥æ£€æŸ¥ç‚¹æ–‡ä»¶æ˜¯å¦å®Œæ•´

## ğŸ“ ä¸‹ä¸€æ­¥

- ğŸ“– [06. æ¨ç†ä½¿ç”¨](./06-æ¨ç†ä½¿ç”¨.md) - ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆå›¾åƒ
- ğŸ“– [07. é…ç½®å‚è€ƒ](./07-é…ç½®å‚è€ƒ.md) - è¯¦ç»†é…ç½®è¯´æ˜

---

**å¼€å§‹è®­ç»ƒ**: è¿è¡Œ `python src/scripts/train.py --config configs/train_config.yaml`

