# 06. æ¨ç†ä½¿ç”¨

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨è®­ç»ƒå¥½çš„ DiT æ¨¡å‹ç”Ÿæˆå›¾åƒã€‚

## ğŸ¨ å¿«é€Ÿå¼€å§‹

### åŸºç¡€æ¨ç†å‘½ä»¤

```bash
python src/scripts/inference.py \
    --checkpoint ./outputs/checkpoint-epoch-10 \
    --prompt "a cat sitting on a chair"
```

### å®Œæ•´å‚æ•°ç¤ºä¾‹

```bash
python src/scripts/inference.py \
    --checkpoint ./outputs/checkpoint-epoch-10 \
    --prompt "a beautiful bird with colorful feathers" \
    --output ./outputs/generated \
    --num_inference_steps 50 \
    --height 256 \
    --width 256 \
    --seed 42
```

## ğŸ“‹ æ¨ç†è„šæœ¬å‚æ•°

### `inference.py` å‚æ•°

**ä½ç½®**: `src/scripts/inference.py`

| å‚æ•° | ç±»å‹ | å¿…éœ€ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| `--checkpoint` | str | âœ… | - | æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ |
| `--prompt` | str | âœ… | - | æ–‡æœ¬æç¤º |
| `--config` | str | âŒ | `configs/train_config.yaml` | é…ç½®æ–‡ä»¶è·¯å¾„ |
| `--output` | str | âŒ | `./outputs/generated` | è¾“å‡ºç›®å½• |
| `--num_inference_steps` | int | âŒ | 50 | æ¨ç†æ­¥æ•° |
| `--height` | int | âŒ | 256 | å›¾åƒé«˜åº¦ |
| `--width` | int | âŒ | 256 | å›¾åƒå®½åº¦ |
| `--seed` | int | âŒ | None | éšæœºç§å­ |

## ğŸ”„ æ¨ç†æµç¨‹è¯¦è§£

### 1. æ¨¡å‹åŠ è½½

**ä»£ç ä½ç½®**: `src/scripts/inference.py` (main å‡½æ•°)

```python
# 1. åŠ è½½é…ç½®
config = load_config(args.config)

# 2. åŠ è½½æ–‡æœ¬ç¼–ç å™¨
text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

# 3. åŠ è½½ VAE è§£ç å™¨
vae_decoder = VAEDecoder(
    pretrained_model_name="runwayml/stable-diffusion-v1-5",
    use_slicing=False,
)

# 4. åˆ›å»º DiT æ¨¡å‹
model = DiTModel(
    hidden_size=config.model.get("hidden_size", 768),
    num_layers=config.model.get("num_layers", 16),
    # ... å…¶ä»–å‚æ•°
)

# 5. åŠ è½½æ£€æŸ¥ç‚¹
checkpoint_path = Path(args.checkpoint) / "model.pt"
state_dict = torch.load(checkpoint_path, map_location=device)
model.load_state_dict(state_dict)

# 6. åˆ›å»ºç”Ÿæˆå™¨
generator = ImageGenerator(
    model=model,
    vae_decoder=vae_decoder,
    text_encoder=text_encoder,
    tokenizer=tokenizer,
    scheduler_type="ddpm",
    device=device,
)
```

### 2. å›¾åƒç”Ÿæˆ

**ä»£ç ä½ç½®**: `src/inference/generator.py` (generate æ–¹æ³•)

```python
@torch.no_grad()
def generate(
    self,
    prompt: str,
    num_inference_steps: int = 50,
    guidance_scale: float = 7.5,
    height: int = 256,
    width: int = 256,
    seed: Optional[int] = None,
) -> Image.Image:
    """
    ç”Ÿæˆå›¾åƒ
    
    æµç¨‹:
    1. ç¼–ç æ–‡æœ¬
    2. åˆå§‹åŒ–æ½œåœ¨å™ªå£°
    3. é€æ­¥å»å™ª
    4. VAE è§£ç 
    5. åå¤„ç†
    """
    # 1. æ–‡æœ¬ç¼–ç 
    text_inputs = self.tokenizer(
        prompt,
        padding="max_length",
        max_length=77,
        return_tensors="pt",
    )
    text_embeddings = self.text_encoder(text_inputs.input_ids.to(self.device))
    text_embeddings = text_embeddings.last_hidden_state.mean(dim=1)  # (B, dim)
    
    # 2. åˆå§‹åŒ–æ½œåœ¨å™ªå£°
    latent_height = height // 8
    latent_width = width // 8
    latents = torch.randn(
        (1, 4, latent_height, latent_width),
        device=self.device,
    )
    # åº”ç”¨ scaling_factorï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    scaling_factor = self.vae_decoder.vae.config.scaling_factor
    latents = latents * scaling_factor
    
    # 3. é€æ­¥å»å™ª
    self.scheduler.set_timesteps(num_inference_steps)
    for t in self.scheduler.timesteps:
        # é¢„æµ‹å™ªå£°
        noise_pred = self.model(latents, t, text_embeddings)
        
        # è°ƒåº¦å™¨æ­¥è¿›
        latents = self.scheduler.step(noise_pred, t, latents).prev_sample
    
    # 4. VAE è§£ç 
    latents = latents / scaling_factor
    image = self.vae_decoder.decode(latents)
    
    # 5. åå¤„ç†
    image = tensor_to_pil_image(image)
    
    return image
```

## âš™ï¸ å‚æ•°è°ƒä¼˜

### æ¨ç†æ­¥æ•° (`num_inference_steps`)

**å½±å“**:
- æ›´å¤šæ­¥æ•° â†’ æ›´é«˜è´¨é‡ï¼Œä½†æ›´æ…¢
- æ›´å°‘æ­¥æ•° â†’ æ›´å¿«ï¼Œä½†è´¨é‡å¯èƒ½ä¸‹é™

**æ¨èå€¼**:
- **å¿«é€Ÿæµ‹è¯•**: 20-30 æ­¥
- **å¹³è¡¡**: 50 æ­¥ï¼ˆé»˜è®¤ï¼‰
- **é«˜è´¨é‡**: 100-200 æ­¥

### å¼•å¯¼å¼ºåº¦ (`guidance_scale`)

**å½±å“**:
- æ›´é«˜å€¼ â†’ æ›´ç¬¦åˆæ–‡æœ¬æè¿°ï¼Œä½†å¯èƒ½è¿‡åº¦é¥±å’Œ
- æ›´ä½å€¼ â†’ æ›´è‡ªç„¶ï¼Œä½†å¯èƒ½åç¦»æ–‡æœ¬

**æ¨èå€¼**:
- **é»˜è®¤**: 7.5
- **æ›´ç¬¦åˆæ–‡æœ¬**: 10-15
- **æ›´è‡ªç„¶**: 3-5

**æ³¨æ„**: å½“å‰å®ç°ä½¿ç”¨ Classifier-Free Guidance (CFG)ï¼Œéœ€è¦åŒæ—¶é¢„æµ‹æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶å™ªå£°ã€‚

### å›¾åƒå°ºå¯¸

**æ”¯æŒå°ºå¯¸**:
- å¿…é¡»æ˜¯ 8 çš„å€æ•°ï¼ˆå› ä¸º VAE ä¸‹é‡‡æ · 8 å€ï¼‰
- æ¨è: 256x256, 512x512

**ä»£ç é™åˆ¶**: `src/inference/generator.py` ä¸­ `latent_height = height // 8`

### éšæœºç§å­

**ç”¨é€”**:
- å›ºå®šç§å­å¯ä»¥å¤ç°ç»“æœ
- ä¸åŒç§å­ç”Ÿæˆä¸åŒå˜ä½“

**ç¤ºä¾‹**:
```bash
# ç”Ÿæˆå¤šä¸ªå˜ä½“
for seed in 42 43 44 45; do
    python src/scripts/inference.py \
        --checkpoint ./outputs/checkpoint-epoch-10 \
        --prompt "a bird" \
        --seed $seed
done
```

## ğŸ¯ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€ç”Ÿæˆ

```bash
python src/scripts/inference.py \
    --checkpoint ./outputs/checkpoint-epoch-10 \
    --prompt "a cat sitting on a chair"
```

### ç¤ºä¾‹ 2: é«˜è´¨é‡ç”Ÿæˆ

```bash
python src/scripts/inference.py \
    --checkpoint ./outputs/checkpoint-epoch-10 \
    --prompt "a beautiful bird with colorful feathers" \
    --num_inference_steps 100 \
    --guidance_scale 10
```

### ç¤ºä¾‹ 3: æ‰¹é‡ç”Ÿæˆ

```bash
# åˆ›å»ºæç¤ºåˆ—è¡¨
prompts=(
    "a cat"
    "a dog"
    "a bird"
    "a person"
)

# æ‰¹é‡ç”Ÿæˆ
for prompt in "${prompts[@]}"; do
    python src/scripts/inference.py \
        --checkpoint ./outputs/checkpoint-epoch-10 \
        --prompt "$prompt" \
        --output ./outputs/generated
done
```

### ç¤ºä¾‹ 4: ä½¿ç”¨ Python API

```python
from src.inference.generator import ImageGenerator
from src.models.dit_model import DiTModel
from src.models.vae_model import VAEDecoder
from transformers import CLIPTextModel, CLIPTokenizer
import torch

# åŠ è½½æ¨¡å‹
device = torch.device("cuda")
model = DiTModel(...).to(device)
vae_decoder = VAEDecoder(...).to(device)
text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")

# åˆ›å»ºç”Ÿæˆå™¨
generator = ImageGenerator(
    model=model,
    vae_decoder=vae_decoder,
    text_encoder=text_encoder,
    tokenizer=tokenizer,
    device=device,
)

# ç”Ÿæˆå›¾åƒ
image = generator.generate(
    prompt="a cat sitting on a chair",
    num_inference_steps=50,
    guidance_scale=7.5,
)

# ä¿å­˜å›¾åƒ
image.save("output.png")
```

## ğŸ” ç”Ÿæˆè´¨é‡ä¼˜åŒ–

### 1. æç¤ºè¯ä¼˜åŒ–

**å¥½çš„æç¤ºè¯**:
- å…·ä½“æè¿°: "a red cat sitting on a wooden chair"
- åŒ…å«ç»†èŠ‚: "a bird with colorful feathers, blue sky background"
- é£æ ¼æè¿°: "photorealistic", "artistic style"

**é¿å…**:
- è¿‡äºç®€å•: "cat"
- çŸ›ç›¾æè¿°: "red blue cat"

### 2. æ¨¡å‹æ£€æŸ¥ç‚¹é€‰æ‹©

**å»ºè®®**:
- ä½¿ç”¨è®­ç»ƒè¾ƒä¹…çš„æ£€æŸ¥ç‚¹ï¼ˆå¦‚ epoch 100+ï¼‰
- æ£€æŸ¥ Loss å€¼ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
- å°è¯•ä¸åŒçš„æ£€æŸ¥ç‚¹ï¼Œé€‰æ‹©æ•ˆæœæœ€å¥½çš„

### 3. åå¤„ç†

ç”Ÿæˆåå¯ä»¥è¿›è¡Œåå¤„ç†ï¼š

```python
from PIL import Image, ImageEnhance

# åŠ è½½ç”Ÿæˆçš„å›¾åƒ
image = Image.open("generated.png")

# å¢å¼ºå¯¹æ¯”åº¦
enhancer = ImageEnhance.Contrast(image)
image = enhancer.enhance(1.2)

# å¢å¼ºé¥±å’Œåº¦
enhancer = ImageEnhance.Color(image)
image = enhancer.enhance(1.1)

# ä¿å­˜
image.save("enhanced.png")
```

## ğŸ› å¸¸è§é—®é¢˜

### é—®é¢˜ 1: ç”Ÿæˆå›¾åƒæ˜¯ç°è‰²çš„

**åŸå› **: æ½œåœ¨ç©ºé—´ç¼©æ”¾å› å­æœªæ­£ç¡®åº”ç”¨

**è§£å†³æ–¹æ¡ˆ**: å·²ä¿®å¤ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°ä»£ç ã€‚æ£€æŸ¥ `src/inference/generator.py` ä¸­çš„ `scaling_factor` åº”ç”¨ã€‚

### é—®é¢˜ 2: ç”Ÿæˆé€Ÿåº¦æ…¢

**ä¼˜åŒ–å»ºè®®**:
- å‡å°‘ `num_inference_steps`ï¼ˆå¦‚ 30 æ­¥ï¼‰
- ä½¿ç”¨è¾ƒå°çš„å›¾åƒå°ºå¯¸
- ç¡®ä¿ä½¿ç”¨ GPU

### é—®é¢˜ 3: ç”Ÿæˆè´¨é‡å·®

**å¯èƒ½åŸå› **:
- æ¨¡å‹è®­ç»ƒä¸å……åˆ†
- æç¤ºè¯ä¸åˆé€‚
- æ¨ç†æ­¥æ•°å¤ªå°‘

**è§£å†³æ–¹æ¡ˆ**:
- ç»§ç»­è®­ç»ƒæ¨¡å‹
- ä¼˜åŒ–æç¤ºè¯
- å¢åŠ æ¨ç†æ­¥æ•°

## ğŸ“ ä¸‹ä¸€æ­¥

- ğŸ“– [07. é…ç½®å‚è€ƒ](./07-é…ç½®å‚è€ƒ.md) - è¯¦ç»†é…ç½®è¯´æ˜
- ğŸ“– [08. æ•…éšœæ’é™¤](./08-æ•…éšœæ’é™¤.md) - æ›´å¤šé—®é¢˜è§£å†³æ–¹æ¡ˆ

---

**å¼€å§‹ç”Ÿæˆ**: è¿è¡Œæ¨ç†è„šæœ¬ï¼Œç”Ÿæˆä½ çš„ç¬¬ä¸€å¼ å›¾åƒï¼
