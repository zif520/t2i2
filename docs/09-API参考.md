# 09. API å‚è€ƒ

æœ¬æŒ‡å—æä¾›é¡¹ç›®æ‰€æœ‰ä¸»è¦ç±»å’Œå‡½æ•°çš„ API æ–‡æ¡£ï¼ŒåŸºäºå®é™…ä»£ç ã€‚

## ğŸ“š æ¨¡å—ç´¢å¼•

- [æ¨¡å‹æ¨¡å—](#æ¨¡å‹æ¨¡å—)
- [æ•°æ®å¤„ç†æ¨¡å—](#æ•°æ®å¤„ç†æ¨¡å—)
- [è®­ç»ƒæ¨¡å—](#è®­ç»ƒæ¨¡å—)
- [æ¨ç†æ¨¡å—](#æ¨ç†æ¨¡å—)
- [å·¥å…·æ¨¡å—](#å·¥å…·æ¨¡å—)

## ğŸ§  æ¨¡å‹æ¨¡å—

### `DiTModel`

**ä½ç½®**: `src/models/dit_model.py`

**ç±»å®šä¹‰**:
```python
class DiTModel(nn.Module):
    def __init__(
        self,
        hidden_size: int = 384,
        num_layers: int = 8,
        num_heads: int = 6,
        patch_size: int = 2,
        in_channels: int = 4,
        out_channels: int = 4,
        attention_head_dim: int = 64,
        mlp_ratio: float = 4.0,
        dropout: float = 0.1,
        input_size: int = 32,
    )
```

**æ–¹æ³•**:

#### `forward(x, t, y)`

å‰å‘ä¼ æ’­

**å‚æ•°**:
- `x` (torch.Tensor): è¾“å…¥æ½œåœ¨è¡¨ç¤ºï¼Œå½¢çŠ¶ `(B, in_channels, H, W)`
- `t` (torch.Tensor): æ—¶é—´æ­¥ï¼Œå½¢çŠ¶ `(B,)`
- `y` (torch.Tensor): æ–‡æœ¬æ¡ä»¶åµŒå…¥ï¼Œå½¢çŠ¶ `(B, text_dim)` æˆ– `(B, seq_len, text_dim)`

**è¿”å›**:
- `torch.Tensor`: é¢„æµ‹çš„å™ªå£°ï¼Œå½¢çŠ¶ `(B, out_channels, H, W)`

### `VAEEncoder`

**ä½ç½®**: `src/models/vae_model.py`

**ç±»å®šä¹‰**:
```python
class VAEEncoder:
    def __init__(
        self,
        pretrained_model_name: str = "runwayml/stable-diffusion-v1-5",
        use_slicing: bool = False,
    )
```

**æ–¹æ³•**:

#### `encode(images)`

å°†å›¾åƒç¼–ç åˆ°æ½œåœ¨ç©ºé—´

**å‚æ•°**:
- `images` (torch.Tensor): å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ `(B, 3, H, W)`ï¼Œå€¼èŒƒå›´ `[-1, 1]`

**è¿”å›**:
- `torch.Tensor`: æ½œåœ¨è¡¨ç¤ºï¼Œå½¢çŠ¶ `(B, 4, H//8, W//8)`

### `VAEDecoder`

**ä½ç½®**: `src/models/vae_model.py`

**ç±»å®šä¹‰**:
```python
class VAEDecoder:
    def __init__(
        self,
        pretrained_model_name: str = "runwayml/stable-diffusion-v1-5",
        use_slicing: bool = False,
    )
```

**æ–¹æ³•**:

#### `decode(latents)`

å°†æ½œåœ¨è¡¨ç¤ºè§£ç ä¸ºå›¾åƒ

**å‚æ•°**:
- `latents` (torch.Tensor): æ½œåœ¨è¡¨ç¤ºï¼Œå½¢çŠ¶ `(B, 4, H, W)`

**è¿”å›**:
- `torch.Tensor`: å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ `(B, 3, H*8, W*8)`ï¼Œå€¼èŒƒå›´ `[-1, 1]`

## ğŸ“Š æ•°æ®å¤„ç†æ¨¡å—

### `TextImageDataset`

**ä½ç½®**: `src/data/dataset.py`

**ç±»å®šä¹‰**:
```python
class TextImageDataset(Dataset):
    def __init__(
        self,
        dataset_name: str = "coco",
        dataset_path: Optional[str] = None,
        image_size: int = 256,
        tokenizer_name: str = "openai/clip-vit-base-patch32",
        max_length: int = 77,
        num_samples: Optional[int] = None,
        is_train: bool = True,
    )
```

**æ”¯æŒçš„æ•°æ®é›†**:
- `"coco"`: COCO æ•°æ®é›†
- `"cub"`: CUB-200-2011 æ•°æ®é›†
- `"custom"`: è‡ªå®šä¹‰æ•°æ®é›†

**æ–¹æ³•**:

#### `__getitem__(idx)`

è·å–æ•°æ®é¡¹

**å‚æ•°**:
- `idx` (int): ç´¢å¼•

**è¿”å›**:
- `Dict[str, torch.Tensor]`: åŒ…å« `pixel_values` å’Œ `input_ids` çš„å­—å…¸

### `get_image_transforms`

**ä½ç½®**: `src/data/transforms.py`

**å‡½æ•°å®šä¹‰**:
```python
def get_image_transforms(
    image_size: int = 256,
    is_train: bool = True,
) -> transforms.Compose
```

**è¿”å›**: å›¾åƒå˜æ¢ç»„åˆ

**å˜æ¢å†…å®¹**:
- è°ƒæ•´å¤§å°åˆ° `image_size`
- ä¸­å¿ƒè£å‰ª
- éšæœºæ°´å¹³ç¿»è½¬ï¼ˆè®­ç»ƒæ—¶ï¼‰
- è½¬æ¢ä¸ºå¼ é‡
- å½’ä¸€åŒ–åˆ° `[-1, 1]`

## ğŸ‹ï¸ è®­ç»ƒæ¨¡å—

### `Trainer`

**ä½ç½®**: `src/training/trainer.py`

**ç±»å®šä¹‰**:
```python
class Trainer:
    def __init__(
        self,
        model: DiTModel,
        vae_encoder: VAEEncoder,
        text_encoder: Any,
        train_dataloader: DataLoader,
        config: Config,
        accelerator: Optional[Accelerator] = None,
    )
```

**æ–¹æ³•**:

#### `train()`

æ‰§è¡Œè®­ç»ƒå¾ªç¯

**åŠŸèƒ½**:
- è®­ç»ƒå¾ªç¯
- è‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹
- æ—¥å¿—è®°å½•

#### `train_step(batch)`

æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤

**å‚æ•°**:
- `batch` (Dict[str, torch.Tensor]): æ‰¹æ¬¡æ•°æ®

**è¿”å›**:
- `Dict[str, float]`: æŸå¤±å­—å…¸

#### `save_checkpoint(checkpoint_dir)`

ä¿å­˜æ£€æŸ¥ç‚¹

**å‚æ•°**:
- `checkpoint_dir` (Path): æ£€æŸ¥ç‚¹ç›®å½•

**ä¿å­˜å†…å®¹**:
- `model.pt`: æ¨¡å‹æƒé‡
- `optimizer.pt`: ä¼˜åŒ–å™¨çŠ¶æ€
- `scheduler.pt`: å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
- `training_state.json`: è®­ç»ƒçŠ¶æ€

#### `load_checkpoint(checkpoint_dir)`

åŠ è½½æ£€æŸ¥ç‚¹

**å‚æ•°**:
- `checkpoint_dir` (Path): æ£€æŸ¥ç‚¹ç›®å½•

**åŠŸèƒ½**:
- åŠ è½½æ¨¡å‹æƒé‡
- æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
- æ¢å¤è®­ç»ƒè¿›åº¦

### `get_scheduler`

**ä½ç½®**: `src/training/scheduler.py`

**å‡½æ•°å®šä¹‰**:
```python
def get_scheduler(
    scheduler_type: str = "ddpm",
    num_train_timesteps: int = 1000,
    beta_start: float = 0.00085,
    beta_end: float = 0.012,
    beta_schedule: str = "scaled_linear",
    prediction_type: str = "epsilon",
) -> DDPMScheduler | DDIMScheduler
```

**è¿”å›**: æ‰©æ•£è°ƒåº¦å™¨å¯¹è±¡

### `DiffusionLoss`

**ä½ç½®**: `src/training/loss.py`

**ç±»å®šä¹‰**:
```python
class DiffusionLoss(nn.Module):
    def __init__(self, loss_type: str = "mse")
    def forward(
        self,
        pred_noise: torch.Tensor,
        target_noise: torch.Tensor,
    ) -> torch.Tensor
```

**æ”¯æŒçš„æŸå¤±ç±»å‹**:
- `"mse"`: å‡æ–¹è¯¯å·®ï¼ˆé»˜è®¤ï¼‰
- `"l1"`: L1 æŸå¤±

## ğŸ¨ æ¨ç†æ¨¡å—

### `ImageGenerator`

**ä½ç½®**: `src/inference/generator.py`

**ç±»å®šä¹‰**:
```python
class ImageGenerator:
    def __init__(
        self,
        model: DiTModel,
        vae_decoder: VAEDecoder,
        text_encoder: Any,
        tokenizer: Any,
        scheduler_type: str = "ddpm",
        device: Optional[torch.device] = None,
    )
```

**æ–¹æ³•**:

#### `generate(prompt, num_inference_steps, guidance_scale, height, width, seed)`

ç”Ÿæˆå›¾åƒ

**å‚æ•°**:
- `prompt` (str): æ–‡æœ¬æç¤º
- `num_inference_steps` (int): æ¨ç†æ­¥æ•°ï¼Œé»˜è®¤ 50
- `guidance_scale` (float): å¼•å¯¼å¼ºåº¦ï¼Œé»˜è®¤ 7.5
- `height` (int): å›¾åƒé«˜åº¦ï¼Œé»˜è®¤ 256
- `width` (int): å›¾åƒå®½åº¦ï¼Œé»˜è®¤ 256
- `seed` (Optional[int]): éšæœºç§å­

**è¿”å›**:
- `Image.Image`: ç”Ÿæˆçš„å›¾åƒ

#### `generate_batch(prompts, ...)`

æ‰¹é‡ç”Ÿæˆå›¾åƒ

**å‚æ•°**:
- `prompts` (List[str]): æ–‡æœ¬æç¤ºåˆ—è¡¨
- å…¶ä»–å‚æ•°åŒ `generate`

**è¿”å›**:
- `List[Image.Image]`: ç”Ÿæˆçš„å›¾åƒåˆ—è¡¨

## ğŸ› ï¸ å·¥å…·æ¨¡å—

### `load_config`

**ä½ç½®**: `src/utils/config.py`

**å‡½æ•°å®šä¹‰**:
```python
def load_config(config_path: str) -> Config
```

**å‚æ•°**:
- `config_path` (str): é…ç½®æ–‡ä»¶è·¯å¾„

**è¿”å›**:
- `Config`: é…ç½®å¯¹è±¡

### `Config`

**ä½ç½®**: `src/utils/config.py`

**ç±»å®šä¹‰**:
```python
@dataclass
class Config:
    data: Dict[str, Any]
    model: Dict[str, Any]
    training: Dict[str, Any]
    scheduler: Dict[str, Any]
    text_encoder: Dict[str, Any]
    optimizer: Dict[str, Any]
    lr_scheduler: Dict[str, Any]
    vae: Optional[Dict[str, Any]] = None
```

**æ–¹æ³•**:

#### `to_dict()`

è½¬æ¢ä¸ºå­—å…¸

**è¿”å›**:
- `Dict[str, Any]`: é…ç½®å­—å…¸

### `setup_logger`

**ä½ç½®**: `src/utils/logger.py`

**å‡½æ•°å®šä¹‰**:
```python
def setup_logger(
    name: str = "dit_tutorial",
    log_file: Optional[Path] = None,
) -> logging.Logger
```

**è¿”å›**: æ—¥å¿—è®°å½•å™¨å¯¹è±¡

### `tensor_to_pil_image`

**ä½ç½®**: `src/utils/visualization.py`

**å‡½æ•°å®šä¹‰**:
```python
def tensor_to_pil_image(tensor: torch.Tensor) -> Image.Image
```

**å‚æ•°**:
- `tensor` (torch.Tensor): å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ `(C, H, W)` æˆ– `(1, C, H, W)`ï¼Œå€¼èŒƒå›´ `[0, 1]`

**è¿”å›**:
- `Image.Image`: PIL å›¾åƒå¯¹è±¡

## ğŸ“ è„šæœ¬ API

### `train.py`

**ä½ç½®**: `src/scripts/train.py`

**å‘½ä»¤è¡Œå‚æ•°**:
- `--config`: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: `configs/train_config.yaml`ï¼‰
- `--resume`: æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¯é€‰ï¼‰

### `inference.py`

**ä½ç½®**: `src/scripts/inference.py`

**å‘½ä»¤è¡Œå‚æ•°**:
- `--checkpoint`: æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- `--prompt`: æ–‡æœ¬æç¤ºï¼ˆå¿…éœ€ï¼‰
- `--config`: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: `configs/train_config.yaml`ï¼‰
- `--output`: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: `./outputs/generated`ï¼‰
- `--num_inference_steps`: æ¨ç†æ­¥æ•°ï¼ˆé»˜è®¤: 50ï¼‰
- `--height`: å›¾åƒé«˜åº¦ï¼ˆé»˜è®¤: 256ï¼‰
- `--width`: å›¾åƒå®½åº¦ï¼ˆé»˜è®¤: 256ï¼‰
- `--seed`: éšæœºç§å­ï¼ˆå¯é€‰ï¼‰

### `prepare_data.py`

**ä½ç½®**: `src/scripts/prepare_data.py`

**å‘½ä»¤è¡Œå‚æ•°**:
- `--type`: æ•°æ®é›†ç±»å‹ï¼ˆ`coco` æˆ– `custom`ï¼‰
- `--output`: è¾“å‡ºç›®å½•
- `--num_samples`: æ ·æœ¬æ•°é‡

### `prepare_coco_from_download.py`

**ä½ç½®**: `src/scripts/prepare_coco_from_download.py`

**å‘½ä»¤è¡Œå‚æ•°**:
- `--images_dir`: COCO å›¾åƒç›®å½•
- `--annotations_file`: COCO æ ‡æ³¨æ–‡ä»¶
- `--output`: è¾“å‡ºç›®å½•
- `--num_samples`: æ ·æœ¬æ•°é‡

### `prepare_cub_from_kaggle.py`

**ä½ç½®**: `src/scripts/prepare_cub_from_kaggle.py`

**å‘½ä»¤è¡Œå‚æ•°**:
- `--kaggle_dir`: Kaggle ä¸‹è½½çš„ CUB æ•°æ®ç›®å½•
- `--output`: è¾“å‡ºç›®å½•
- `--num_samples`: æ ·æœ¬æ•°é‡
- `--use_train`: ä½¿ç”¨è®­ç»ƒé›†ï¼ˆé»˜è®¤ï¼‰
- `--use_val`: ä½¿ç”¨éªŒè¯é›†

## ğŸ“ ä¸‹ä¸€æ­¥

- ğŸ“– [01. å¿«é€Ÿå¼€å§‹](./01-å¿«é€Ÿå¼€å§‹.md) - ä½¿ç”¨è¿™äº› API
- ğŸ“– [05. è®­ç»ƒæŒ‡å—](./05-è®­ç»ƒæŒ‡å—.md) - è®­ç»ƒæµç¨‹
- ğŸ“– [06. æ¨ç†ä½¿ç”¨](./06-æ¨ç†ä½¿ç”¨.md) - æ¨ç†æµç¨‹

---

**API æŸ¥è¯¢**: æŸ¥çœ‹æºç  `src/` ç›®å½•è·å–æ›´è¯¦ç»†çš„å®ç°ç»†èŠ‚ï¼

