# 混合精度训练指南：FP16 vs BF16

## 📊 快速对比

| 特性 | FP16 (float16) | BF16 (bfloat16) |
|------|----------------|-----------------|
| **动态范围** | 较小（可能溢出） | 大（接近 FP32） |
| **指数位** | 5 位 | 8 位 |
| **尾数位** | 10 位 | 7 位 |
| **稳定性** | 需要 loss scaling | 更稳定 |
| **兼容性** | 所有现代 GPU | Ampere+ (RTX 3090, A100, 4090等) |
| **速度** | 快 | 快（与 FP16 相当） |

## 🎯 推荐选择

### RTX 4090（你的 GPU）

**✅ 推荐使用 BF16**

原因：
1. **RTX 4090 支持 BF16**（Ampere 架构）
2. **训练更稳定**：动态范围大，不易溢出
3. **通常不需要 loss scaling**：简化训练流程
4. **性能相当**：与 FP16 速度相近
5. **更好的数值稳定性**：减少 NaN/Inf 问题

### 其他 GPU

- **Ampere 或更新**（RTX 3090, A100, RTX 4090等）：推荐 BF16
- **较老的 GPU**（V100, RTX 2080等）：使用 FP16

## 🔧 配置方法

### 使用 BF16（推荐）

```yaml
training:
  mixed_precision: "bf16"
```

### 使用 FP16

```yaml
training:
  mixed_precision: "fp16"
```

## ⚠️ 注意事项

### FP16 的问题

1. **溢出风险**：动态范围小，梯度可能溢出
2. **需要 loss scaling**：通常需要手动调整
3. **数值不稳定**：可能出现 NaN/Inf

### BF16 的优势

1. **不易溢出**：动态范围接近 FP32
2. **更稳定**：通常不需要 loss scaling
3. **更好的收敛**：数值特性更接近 FP32

## 📈 实际效果

### 训练稳定性

- **FP16**: 可能需要调整 loss scaling，偶尔出现数值问题
- **BF16**: 更稳定，很少出现数值问题

### 训练速度

- **两者相当**：在现代 GPU 上性能相近
- **显存使用**：相同（都是 16 位）

### 模型质量

- **BF16**: 通常略好（更稳定的训练）
- **FP16**: 如果配置得当，效果也良好

## 💡 建议

### 对于 RTX 4090

**强烈推荐 BF16**：
- ✅ 更稳定
- ✅ 更简单（不需要 loss scaling）
- ✅ 性能相当
- ✅ 更好的数值特性

### 如果遇到问题

如果使用 BF16 遇到问题（很少见），可以：
1. 检查 GPU 是否支持：`torch.cuda.is_bf16_supported()`
2. 回退到 FP16
3. 检查代码中的数值操作

## 🔍 检查 GPU 支持

```python
import torch

if torch.cuda.is_bf16_supported():
    print("✓ 支持 BF16，推荐使用")
else:
    print("✗ 不支持 BF16，使用 FP16")
```

## 📝 总结

**对于 RTX 4090：使用 BF16**

- 更稳定
- 更简单
- 性能相当
- 更好的训练效果

配置文件已更新为使用 BF16。

